# Codes-for-Algorithms
- Codes for realizing theories learned from Data Mining, Machine Learning, Deep Learning without using the present Python packages. (Just basic coding test to challenge myself).
- **Upload on a random basis.**

--------

1. Junior Data Processing
  - Calculate Distance **N dimensional vector**:
    - [x] euclidean
    - [x] manhattan
    - [x] Minkowski
    - [x] Chebyshev
  - Calculate Similarity: 
    - [x] Jaccard
    - [x] Simple Matching Coefficent(SMC) 
  - Calculate relevance:
    - [x] Covariance
    - [x] Sample Standard Deviation (ssd)
    - [x] Pearson Coeffiencent 
2. Models
  - Linear
    - [x] Batch Gradient Descent Regression 
    - Features:
      1. with **Square Loss Function**
      2. Fit for Simple Linear Regression &ã€€Multi-variate Regression
      4. Plot Loss Function & Compare Yhat and YReal

    - [ ] Mini Batch Gradient Descent Regression **Working on Progress** ğŸ¦¾
  - Classification
    - Decision Tree 
     - [x] Entropy: Compute the uncertainty of DateSet 
     - [ ] Information Gain
     - [ ] Decision Tree Visualization **Working on Progress** ğŸ¦¾
     - [x] ID3 
     - [ ] C4.5 **Working on Progress** ğŸ¦¾
    - Bayes
     - [ ] Naive Bayes
       - å¯¹å±æ€§é›†å’Œç±»å˜é‡çš„æ¦‚ç‡å»ºæ¨¡ï¼Œ**ç›‘ç£**å­¦ä¹ 
       - å‡å®šä¸€ä¸ªå±æ€§å¯¹ç»™å®šç±»çš„å½±å“ç‹¬ç«‹äºå…¶ä»–å±æ€§çš„å€¼ï¼Œä¸å­˜åœ¨ä¾èµ–å…³ç³»
       - ç®—æ³•ï¼š
         - åˆ©ç”¨è´å¶æ–¯å…¬å¼è®¡ç®—æ¯ä¸ªç±»åˆ«çš„åéªŒæ¦‚ç‡ P(Y=yi|X) = P(X|Y=yi)*P(Y=yi)/P(X)
         - é€‰æ‹©ä½¿å¾—åéªŒæ¦‚ç‡æœ€å¤§çš„ç±»yi
         - æ¡ä»¶ç‹¬ç«‹æ€§ï¼šå‡è®¾å±æ€§ä¹‹é—´ç‹¬ç«‹ï¼šP(X|Y=yiï¼‰ = PI(P(X=xi|Y=yi))
            - P(Y=yi|X) = argmax{Y=Ci}(P(X|Y=yi)*P(Y=yi)/P(X))
         - åéªŒæ¦‚ç‡æœ€å¤§åŒ–ï¼Œç­‰ä»·äºæœŸæœ›é£é™©æœ€å°åŒ–
            - 0-1æŸå¤± Rï¼ˆfï¼‰= E(L(Y,f(X))
            - æ¡ä»¶æœŸæœ›ï¼šR(f) = Ex(sum(L(yi,f))*P(yi|X))
            - æ¡ä»¶æœŸæœ›æœ€å°åŒ–ï¼Œå¯ä»¥ç»“åˆ0-1æŸå¤±å‡½æ•°è½¬åŒ–ä¸ºåéªŒæ¦‚ç‡æœ€å¤§åŒ–åŸåˆ™
        - æ–¹æ³•
          - æå¤§ä¼¼ç„¶ä¼°è®¡
          - è´å¶æ–¯ä¼°è®¡
            1. è¿ç»­å±æ€§çš„ç±»æ¡ä»¶æ¦‚ç‡ä¼°è®¡ï¼šå°†è¿ç»­å±æ€§ç¦»æ•£åŒ–ï¼Œç”¨ç›¸åº”çš„ç¦»æ•£å–ä»¶æ›¿æ¢è¿ç»­å±æ€§å€¼ï¼Œé€šè¿‡è®¡ç®—ç±»ä¸çš„è®­ç»ƒè®°å½•ä¸­è½å…¥æŒ‡å®šå–ä»¶çš„æ¯”ç‡æ¥ä¼°è®¡æ¡ä»¶æ¦‚ç‡
            2. äºŒåˆ†åˆ’åˆ†ï¼šç»™æ–°å±æ€§è®¾ç½®ä¸€ä¸ªåˆ’åˆ†
            3. æ¦‚ç‡å¯†åº¦ä¼°è®¡ï¼šå‡å®šå±æ€§æœä»ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œåˆ©ç”¨è®­ç»ƒæ•°æ®ä¼°è®¡åˆ†å¸ƒçš„å‚æ•°ï¼Œç¡®å®šåˆ†å¸ƒä¹‹ååˆ©ç”¨åˆ†å¸ƒä¼°è®¡ç±»çš„æ¡ä»¶æ¦‚ç‡
        - ç®—æ³•æµç¨‹
          1. ç¡®å®šç‰¹å¾å±æ€§
          2. è·å–è®­ç»ƒé›†
          3. å¯¹ç±»åˆ«è®¡ç®—å…ˆéªŒæ¦‚ç‡
          4. å¯¹æ¯ä¸ªç‰¹å¾å±æ€§å±æ€§å€¼è®¡ç®—æ‰€æœ‰åˆ’åˆ†çš„æ¡ä»¶æ¦‚ç‡
          5. ç”¨å…ˆéªŒæ¦‚ç‡å’Œæ¡ä»¶æ¦‚ç‡è®¡ç®—åä»°æ¦‚ç‡ï¼Œå°†argmaxä½œä¸ºç±»åˆ«è¾“å‡º
          -----
         - [x] åˆçº§ç‰ˆèˆ†æƒ…ç›‘æµ‹ 
    - Sigmoid
     - [ ] Logistic Regression **Working on Progress** ğŸ¦¾
  - Cluster
    - [x] K-Nearest Neighbor
        - Kè¿‘é‚»ç®—æ³•
          - â€œç‰©ä»¥ç±»èšï¼Œäººä»¥ç¾¤åˆ†â€æ€æƒ³
          - â€œå°‘æ•°æœä»å¤šæ•°â€æ€æƒ³
            - è‹¥æ— æ³•åˆ¤å®šå±äºå“ªä¸ªç±»ï¼Œå°±å½’ä¸ºæƒé‡å¤§çš„ç±»
        - è¦ç´ 
          1. è·ç¦»
          2. Kå€¼
            - Kå€¼é€‰æ‹©
            1. If Kå€¼è¾ƒå° then è·ç¦»å¾ˆè¿‘æ‰èƒ½è¿›è¡Œé¢„æµ‹ï¼Œç”±äºå…·æœ‰å™ªå£°ï¼Œæ•´ä½“æ¨¡å‹è¿‡äºå¤æ‚ï¼Œä¼šå‘ç”Ÿè¿‡æ‹Ÿåˆç°è±¡
            2. IF Kå€¼è¾ƒå¤§ then è¿‘ä¼¼è¯¯å·®ä¼šè¾ƒå¤§ï¼Œä¸ç›¸ä¼¼ç‚¹å½’ä¸ºåŒä¸€ç±»ï¼Œé¢„æµ‹é”™è¯¯ï¼Œæ¨¡å‹è¿‡äºç®€å•
            3. If K = Nï¼Œä¿¡æ¯å¤§é‡æŸå¤±ï¼Œæ¨¡å‹è¿‡äºç®€å•
              - ç”¨cross validation æ¥é€‰æ‹©æœ€ä¼˜çš„kå€¼
          3. åˆ†ç±»å†³ç­–è§„åˆ™
          - å¤šæ•°è¡¨å†³æ³•
            - æ³¨ï¼šè®¡ç®—è·ç¦»æ—¶på€¼ä¸åŒæœ€é‚»è¿‘ç‚¹ä¼šä¸åŒã€‚
        - ç®—æ³•åŸç†
          - input: å¸¦ç±»æ ‡è®°çš„æ ·æœ¬æ•°æ®é›†åˆï¼›æ²¡æœ‰æ ‡ç­¾çš„æ•°æ®
          - process: å°†å‹æ•°æ®çš„æ¯ä¸ªç‰¹å¾ä¸æ ·æœ¬é›†çš„ç‰¹å¾æ¯”è¾ƒï¼Œæå–å‰Kä¸ªæœ€ç›¸ä¼¼çš„æ ‡ç­¾
          - output: æ–°åˆ†ç±»ç»“æœ
          1. è®¡ç®—trainingä¸­çš„ç‚¹ä¸å½“å‰ç‚¹çš„è·ç¦»
          2. æŒ‰è·ç¦»é€’å¢æ’åº
          3. é€‰å–ä¸å½“å‰ç‚¹è·ç¦»æœ€å°çš„kä¸ªç‚¹
          4. ç»Ÿè®¡å‰kä¸ªç‚¹ä¸­æ¯ä¸€ç±»çš„é¢‘ç‡
          5. è¿”å›é¢‘ç‡æœ€å¤§çš„trainingä¸­çš„ç‚¹çš„ç±»åˆ«
          6. ç»Ÿè®¡åˆ†ç±»é”™è¯¯ç‡
        - Tricks for Data Analysis
          - è¾“å‡ºåˆ†ç±»ç»“æœä¹‹åï¼Œåšåˆ†ç±»é”™è¯¯ç‡ç»Ÿè®¡
          - æ•°æ®é¢„å¤„ç†-> æ•°æ®å¯è§†åŒ–ï¼šåŸå§‹æ•°æ®çš„åˆ†ç±»ç»“æœ
          - ç”¨Kè¿‘é‚»å»ºæ¨¡ï¼Œè®¡ç®—é”™è¯¯ç‡
          - çœ‹é”™è¯¯ç‡è°ƒæ•´Kå€¼
            - Kè¿‘é‚»å—æ•°å€¼çš„å½±å“è¾ƒå¤§ï¼Œå¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–å¤„ç† new = (old - min)/(max - min)
            - train_test_split,æ£€éªŒå‡†ç¡®ç‡
            - è°ƒæ•´æµ‹è¯•æ•°æ®æ¯”ä¾‹æˆ–è€…Kæ¥è°ƒå‚
#### <div align="center"><font color='#00338D'>â€œæˆ‘æ— åšä¸æ‘§ å°†æƒ…è—å¾—éšæ™¦ åªæœ‰é‚£æ˜æœˆ çŸ¥é“è¿‡ç¨‹å£®çƒˆâ€</font></div> 
    - [ ] KMeans
    - [x] Apriori
    - [x] FPGrowth  
  - Neural Network **Working on Progress** ğŸ‘»ğŸ‘»ğŸ‘»ğŸ‘»

---
- **Hope that there's 0.1% possibility that I could start my journey to Computer Vision.**
  - By Tracy Tao (Dasein), a STEM GIRL in business world.
---
Post Scrpits:
- By the way, I wanna shout out loud that I love Elon Musk! ğŸ§ 

    
