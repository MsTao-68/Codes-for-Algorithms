{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1edb8b9-1e53-40d5-9184-c659b78f2667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !usr/bin/python\n",
    "# -*- encoding: utf-8 -*-\n",
    "# Project: Sequence to Squence - Translation\n",
    "# Author: Tracy Tao\n",
    "# Date: 2022/04/13\n",
    "'''\n",
    "建一个基于LSTM的Seq2Seq模型，使用编码器-解码器架构进行机器翻译\n",
    "源语言(SRC - Input)是德语，目标语言(TRG - Output)是英语\n",
    "“序列开始”<sos>和“序列结束”<eos>\n",
    "'''\n",
    "# 系统操作\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random \n",
    "from io import open\n",
    "\n",
    "# pyrotch框架\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as DataSet\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75f5cc6b-cbac-4aa1-992e-02ab01b85397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义标识符 <bos> <eos>\n",
    "sos_token = 0\n",
    "eos_token = 1\n",
    "\n",
    "# read corpus\n",
    "fra_path = 'E:\\\\dasein_py\\\\Data Analysis\\\\Deep Learning - Pytorch\\\\Deep Learning - Pytorch\\\\机器翻译的神经网络实现\\\\data\\\\fra.txt'\n",
    "eng_path = 'E:\\\\dasein_py\\\\Data Analysis\\\\Deep Learning - Pytorch\\\\Deep Learning - Pytorch\\\\机器翻译的神经网络实现\\\\data\\\\eng.txt'\n",
    "lines = open(fra_path, encoding = 'utf-8')\n",
    "french = lines.read().strip().split('\\n') # 去空格，按行切分\n",
    "lines = open(eng_path, encoding = 'utf-8')\n",
    "english = lines.read().strip().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49f99718-353c-4b48-b6b8-caa016f60af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135842, 135842)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english),len(french)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ece0ef54-2c9c-4101-86d2-54ee5015d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language:\n",
    "    '''定义语言类：建词典、编码、单词到索引、索引到单词、设置标识符'''\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: 'SOS', 1: 'EOS'} # 标识符索引\n",
    "        self.n_words = 2 # count sos & eos\n",
    "        \n",
    "    def add_sentence(self, sentence):\n",
    "        '''在语言中添加新句子，然后切分获得单词'''\n",
    "        for word in sentence.split(' '): # tokenize sentence\n",
    "            self.add_word(word)\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        '''添加单词，并且更新字典中的词频；并且建立反向索引'''\n",
    "        if self.word2index.get(word,0) == 0: # if word not in self.word2index\n",
    "            self.word2index[word] = self.n_words # 索引\n",
    "            self.word2count[word] = 1 # 计数\n",
    "            self.index2word[self.n_words] = word # 反向索引，给单词按顺序编号\n",
    "            self.n_words +=1 # 记录单词数\n",
    "        else:\n",
    "            self.word2count[word] += 1 # 如果存在直接计数        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a9c8897-5408-42aa-a1fa-e8bfc382c9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def UnicodeToAscii(string):\n",
    "    '''\n",
    "    Unicode -> Ascii \n",
    "    NFD表示字符应该分解为多个组合字符表示\n",
    "    '''\n",
    "    return ''.join(ch for ch in unicodedata.normalize('NFD',string) if unicodedata.category(ch) != 'Mn') # 文本标准化\n",
    "\n",
    "def LowerEngString(string):\n",
    "    '''\n",
    "    function: Eng.lower()\n",
    "    usage: re.sub(pattern, repl, string, count=0, flags=0)\n",
    "    '''\n",
    "    s = UnicodeToAscii(string.lower().strip())   # Unicode -> Ascii \n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s) # 去除标点符号\n",
    "    return s\n",
    "\n",
    "\n",
    "MAX_LENGTH = 5 # 设置句子的最大长度\n",
    "def FilterPair(pair):\n",
    "    '''\n",
    "    function: 对输入单词对过滤，限制每句话的单词数\n",
    "    '''\n",
    "    return len(pair[0].split(' ')) < MAX_LENGTH and len(pair[1].split(' ')) < MAX_LENGTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34b99c2d-dbd6-4111-832f-73dbfbd50750",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IndexesFromSentence(class_, sentence):\n",
    "    '''\n",
    "    function: 输入句子，按空格分词之后对单词索引\n",
    "    return：单词索引序列\n",
    "    '''\n",
    "    return [class_.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def IndexFromSentence(class_, sentence):\n",
    "    '''\n",
    "    function: 输入句子，按空格分词之后对单词索引\n",
    "    return: 输出的序列==MAX_LENGTH\n",
    "    '''\n",
    "    idx = IndexesFromSentence(class_, sentence)  # 单词索引序列\n",
    "    idx.append(eos_token) # 在结尾增添eos索引\n",
    "    for i in range(MAX_LENGTH - len(idx)):\n",
    "         idx.append(eos_token) # 不知所云\n",
    "    return idx\n",
    "\n",
    "def IndexFromPair(pair):\n",
    "    '''从一个词对到下标, return: 下标对'''\n",
    "    input_variable = IndexFromSentence(input_lang, pair[0])\n",
    "    target_variable = IndexFromSentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)\n",
    "\n",
    "def SentenceFromList(class_, lst):\n",
    "    '''单词列表 -> 句子'''\n",
    "    result = [class_.index2word[i] for i in lst if i != eos_token]\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b63c025a-585f-462d-816a-92fbbc8225ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_acc(pred, labels):\n",
    "    '''\n",
    "    :params pred: predictions\n",
    "    :params labels: y_true\n",
    "    function: 计算正确率\n",
    "    return: （正确数, 总计）\n",
    "    '''\n",
    "    pred = torch.max(pred.data, 1)[1] #  对于任意一行（一个样本）的输出值的第1个维度，将最大的作为预测结果\n",
    "    acc = pred.eq(labels.data).sum() # pred与labels比较，并累计得到正确数\n",
    "    return acc, len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "149b7e7a-3044-425a-ba37-91fb4446c02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#语料标准化处理，将每一行进行zip\n",
    "pairs = [[LowerEngString(fra), LowerEngString(eng)] for fra, eng in zip(french, english)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "52ca60bc-dd84-40a1-a3af-7e9df893475d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['va  !', 'go .'],\n",
       " ['cours\\u202f !', 'run !'],\n",
       " ['courez\\u202f !', 'run !'],\n",
       " ['ca alors\\u202f !', 'wow !'],\n",
       " ['au feu  !', 'fire !']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39689894-c51b-4097-9279-ae59c472f890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有效句子对： 7129\n"
     ]
    }
   ],
   "source": [
    "input_lang = Language('French')\n",
    "output_lang = Language('English')\n",
    "pairs = [pair for pair in pairs if FilterPair(pair)]\n",
    "print('有效句子对：', len(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc188b0e-f452-4f57-bd8a-8e42094857d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总单词数:\n",
      "French: 4202\n",
      "English: 2278\n"
     ]
    }
   ],
   "source": [
    "# 建立两个词典（法文和英文的）\n",
    "for pair in pairs:\n",
    "    input_lang.add_sentence(pair[0])\n",
    "    output_lang.add_sentence(pair[1])\n",
    "print(\"总单词数:\\n{}: {}\\n{}: {}\".format(input_lang.name, input_lang.n_words, output_lang.name, output_lang.n_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03a4ab99-77f9-4335-873b-e02d295b2516",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idx = np.random.permutation(range(len(pairs)))\n",
    "pairs = [pairs[i] for i in random_idx]  # 打乱所有训练集句子的顺序\n",
    "pairs = [IndexFromPair(pair) for pair in pairs] # 语言转变为单词的编码构成的序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fc7f0b1-3696-4d45-905f-bb6b27204020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[([119, 208, 4104, 14, 1], [104, 765, 2195, 3, 1]),\n",
       " ([20, 226, 530, 14, 1], [65, 82, 93, 3, 1]),\n",
       " ([7, 2201, 14, 1, 1], [190, 1107, 50, 3, 1]),\n",
       " ([2405, 547, 3929, 14, 1], [544, 475, 2074, 3, 1]),\n",
       " ([20, 44, 666, 14, 1], [12, 173, 224, 3, 1])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba8b01c5-60eb-433c-aeb1-c558af76a0c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "712"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_size = len(pairs) // 10\n",
    "if valid_size > 10000:\n",
    "    valid_size = 10000\n",
    "valid_size # validation set size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c0c1ccf-971c-46b5-b63f-b46a471eb48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练记录： 6417\n",
      "校验记录： 356\n",
      "测试记录： 356\n"
     ]
    }
   ],
   "source": [
    "pp = pairs\n",
    "pairs = pairs[ : - valid_size] # 训练集\n",
    "valid_pairs = pp[-valid_size : -valid_size // 2] # 验证集集\n",
    "test_pairs = pp[- valid_size // 2 :] # 测试集\n",
    "print('训练记录：', len(pairs))\n",
    "print('校验记录：', len(valid_pairs))\n",
    "print('测试记录：', len(test_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f75f12e5-7086-4cec-861e-7c12612e4a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e3d77f9-fe20-4ada-9cd8-05ce88f992c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[119, 208, 4104, 14, 1],\n",
       "  [20, 226, 530, 14, 1],\n",
       "  [7, 2201, 14, 1, 1],\n",
       "  [2405, 547, 3929, 14, 1],\n",
       "  [20, 44, 666, 14, 1]],\n",
       " [[104, 765, 2195, 3, 1],\n",
       "  [65, 82, 93, 3, 1],\n",
       "  [190, 1107, 50, 3, 1],\n",
       "  [544, 475, 2074, 3, 1],\n",
       "  [12, 173, 224, 3, 1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs_X = [pair[0] for pair in pairs]\n",
    "pairs_Y = [pair[1] for pair in pairs]\n",
    "pairs_X[:5], pairs_Y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "575863af-ecb7-4ebb-8bce-5785e6ba3871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[401, 344, 2457, 14, 1],\n",
       "  [2393, 128, 7, 14, 1],\n",
       "  [174, 2784, 14, 1, 1],\n",
       "  [57, 802, 1660, 14, 1],\n",
       "  [342, 934, 3744, 14, 1]],\n",
       " [[401, 344, 2457, 14, 1],\n",
       "  [2393, 128, 7, 14, 1],\n",
       "  [174, 2784, 14, 1, 1],\n",
       "  [57, 802, 1660, 14, 1],\n",
       "  [342, 934, 3744, 14, 1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_X = [pair[0] for pair in valid_pairs]\n",
    "valid_Y = [pair[1] for pair in valid_pairs]\n",
    "valid_X[:5], valid_X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a55b41bf-db46-48b8-8f02-8bd5c76d182e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[119, 208, 484, 14, 1],\n",
       "  [23, 1975, 843, 14, 1],\n",
       "  [268, 208, 3032, 14, 1],\n",
       "  [2566, 2567, 3, 4, 1],\n",
       "  [268, 208, 1011, 14, 1]],\n",
       " [[60, 192, 1188, 3, 1],\n",
       "  [12, 408, 427, 3, 1],\n",
       "  [146, 192, 1187, 3, 1],\n",
       "  [71, 194, 1295, 3, 1],\n",
       "  [146, 192, 489, 3, 1]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X = [pair[0] for pair in test_pairs]\n",
    "test_Y = [pair[1] for pair in test_pairs]\n",
    "test_X[:5], test_Y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e01277f-58bf-45b3-a2b8-249a5128d726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 119,  208, 4104,   14,    1],\n",
       "         [  20,  226,  530,   14,    1],\n",
       "         [   7, 2201,   14,    1,    1],\n",
       "         [2405,  547, 3929,   14,    1],\n",
       "         [  20,   44,  666,   14,    1]]),\n",
       " tensor([[ 104,  765, 2195,    3,    1],\n",
       "         [  65,   82,   93,    3,    1],\n",
       "         [ 190, 1107,   50,    3,    1],\n",
       "         [ 544,  475, 2074,    3,    1],\n",
       "         [  12,  173,  224,    3,    1]]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 利用PyTorch的dataset和dataloader，将数据加载到加载器里面，并且分批\n",
    "# 形成训练集, torch.LongTensor是64位整型\n",
    "train_dataset = DataSet.TensorDataset(torch.LongTensor(pairs_X), torch.LongTensor(pairs_Y)) \n",
    "train_dataset[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cd9a534-8b27-4b27-872d-37d1de07319c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x214a9b66cd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 形成数据加载器, 打乱顺序\n",
    "train_loader = DataSet.DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers=8)\n",
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34848027-72b7-4da2-88f0-f2eed56618d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x214a9b66c40>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 验证集\n",
    "valid_dataset = DataSet.TensorDataset(torch.LongTensor(valid_X), torch.LongTensor(valid_Y))\n",
    "valid_loader = DataSet.DataLoader(valid_dataset, batch_size = batch_size, shuffle = True, num_workers=8)\n",
    "valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cd2f75d-ba92-425f-8e41-8035b5637c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x214abfb22e0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试集\n",
    "test_dataset = DataSet.TensorDataset(torch.LongTensor(test_X), torch.LongTensor(test_Y))\n",
    "test_loader = DataSet.DataLoader(test_dataset, batch_size = batch_size, shuffle = True, num_workers = 8)\n",
    "test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4de4c617-f0a6-45b7-bae8-b8bb752ff432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder -> Decoder\n",
    "# Encoder: bi-GRU: bidirectional = True \n",
    "# 在编码器中，输入层为一个嵌入（embedding）层，它将每个输入的法文词的 one-hot 编码转化为一个词向量。\n",
    "# RNN对句子的词语进行循环编码\n",
    "# 编码器没有输出层，将 RNN全部隐含单元的输出状态作为解码器输入\n",
    "class EncoderRNN(nn.Module):\n",
    "    '''Encoder - bi-GRU'''\n",
    "    def __init__(self, input_size, hidden_size, num_layers = 1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size) \n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first = True,\n",
    "                          num_layers = self.num_layers, bidirectional = True)\n",
    "        # batch_first, 将batch_size放在第一维\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        '''\n",
    "        function: feed_forword\n",
    "        :param input: (batch_size, length_seq)\n",
    "        :param hidden: (num_layers * directions, batch_size, hidden_size)\n",
    "        elem: output：(batch_size, length_seq, hidden_size)\n",
    "        return: output, hidden\n",
    "        '''\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        '''\n",
    "        function: 隐含单元初始化\n",
    "        return result: (num_layers * num_directions, batch, hidden_size)\n",
    "        '''\n",
    "        result = Variable(torch.zeros(self.num_layers * 2, batch_size, self.hidden_size))\n",
    "        return result.cuda() if use_cuda else result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d97f8623-583b-4e87-a961-ef820058db14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "# 解码器需要设计损失函数\\反向传播算法\n",
    "# 一个嵌入（embedding）层用于将输入的 one-hot 向量转化为词向量，与编码器架构相同的循环神经网络（RNN或LSTM）\n",
    "# softmax 运算来确定所对应的目标语言单词，和计算模型的损失（Loss）以完成反向传播的过程\n",
    "class DecoderRNN(nn.Module):\n",
    "    '''Decoder - bi-GRU'''\n",
    "    def __init__(self, hidden_siz, output_size, num_layers =1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first = True,\n",
    "                        num_layers = self.num_layers, bidirectional = True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size) # 全链接层\n",
    "        self.softmax = nn.LogSoftmax(dim = 1)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input) \n",
    "        # input：(batch_size, length_seq)\n",
    "        # embedded: (batch_size, length_seq, hidden_size)\n",
    "        output = F.relu(embedded)  # 平滑处理\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.dropout(output)  \n",
    "        # output：(batch_size, length_seq, hidden_size * directions)\n",
    "        # hidden：(n_layers * directions, batch_size, hidden_size)\n",
    "        output = self.softmax(self.out(output[:, -1, :]))\n",
    "        # (batch_size * output_size)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        '''\n",
    "        function: 隐含单元初始化\n",
    "        return result: (num_layers * num_directions, batch, hidden_size)\n",
    "        '''\n",
    "        result = Variable(torch.zeros(self.num_layers * 2, batch_size, self.hidden_size))\n",
    "        return result.cuda() if use_cuda else result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9889ab6d-697f-4363-b371-aa74a6d7dd5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(EncoderRNN(\n",
       "   (embedding): Embedding(4202, 32)\n",
       "   (gru): GRU(32, 32, batch_first=True, bidirectional=True)\n",
       " ),\n",
       " DecoderRNN(\n",
       "   (embedding): Embedding(2278, 32)\n",
       "   (gru): GRU(32, 32, batch_first=True, bidirectional=True)\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (out): Linear(in_features=64, out_features=2278, bias=True)\n",
       "   (softmax): LogSoftmax(dim=1)\n",
       " ))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_size = 32\n",
    "max_length = MAX_LENGTH\n",
    "n_layers = 1\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size, num_layers = n_layers)\n",
    "decoder = DecoderRNN(hidden_size, output_lang.n_words, num_layers = n_layers)\n",
    "encoder, decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63c33972-377b-4cf0-81e1-2c267a824922",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_cuda:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10fd5757-ff7e-4216-9767-ef6a33f45424",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "# 为两个网络分别定义优化器\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb6fc224-2d2c-4725-8cc9-5a5d30ee8d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数 CrossEntropyLoss()=log_softmax() + NLLLoss() \n",
    "criterion = nn.NLLLoss()\n",
    "# 有 50% 的概率选用这种直接使用正确答案的监督学习方式\n",
    "teacher_forcing_ratio = 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e30d3778-58c2-421f-a2ef-b9f648b1110b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用于记录训练中的损失信息，后面绘制图像用\n",
    "plot_losses = []\n",
    "print_loss_total = 0\n",
    "print_loss_avg = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52fe70b1-64af-4044-832a-2301609f8ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    global plot_losses\n",
    "    global print_loss_total\n",
    "    global print_loss_avg\n",
    "    print_loss_total = 0\n",
    "    \n",
    "    for data in train_loader:\n",
    "        input_variable = Variable(data[0]).cuda() if use_cuda else Variable(data[0])\n",
    "        # input_variable: (batch_size, length_seq)\n",
    "        target_variable = Variable(data[1]).cuda() if use_cuda else Variable(data[1])\n",
    "        # target_variable: (batch_size, length_seq)\n",
    "        \n",
    "        encoder_hidden = encoder.initHidden(data[0].size()[0]) # 初始化编码器状态\n",
    "        \n",
    "        encoder_optimizer.zero_grad() # 清空梯度\n",
    "        decoder_optimizer.zero_grad() # 清空梯度\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "        # encoder_outputs: (batch_size, length_seq, hidden_size*direction)\n",
    "        # encoder_hidden: (direction*n_layer, batch_size, hidden_size)\n",
    "        \n",
    "        decoder_input = Variable(torch.LongTensor([[sos_token]] * target_variable.size()[0]))\n",
    "        # decoder_input: (batch_size, length_seq)\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "        # decoder_input: (batch_size, length_seq)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden # 继承encoder隐藏单元\n",
    "        # decoder_hidden: (direction*n_layer, batch_size, hidden_size)\n",
    "        \n",
    "        # teacher_forcing_ratio： 用target中的翻译结果作为监督信息 - 50%\n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "        base = torch.zeros(target_variable.size()[0]) \n",
    "        \n",
    "        if use_teacher_forcing: # 如果使用教师监督: 将下一个时间步的监督信息输入给解码器\n",
    "            for di in range(MAX_LENGTH):  # 对时间步循环\n",
    "                \n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                # decoder_ouput：(batch_size, output_size)\n",
    "                \n",
    "                loss += criterion(decoder_output, target_variable[:, di]) # 累计损失\n",
    "                \n",
    "                decoder_input = target_variable[:, di].unsqueeze(1) # 将训练数据当做下一时间步的输入\n",
    "                # decoder_input: (batch_size, length_seq)\n",
    "                \n",
    "        else: # 反之，使用解码器自己的预测作为下一时间步的输入\n",
    "            for di in range(MAX_LENGTH): # 对时间步循环\n",
    "                \n",
    "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                #decoder_ouput：(batch_size, output_size(input_size))\n",
    "                \n",
    "                topv, topi = decoder_output.data.topk(1, dim = 1) # 从输出结果中选择出一个数值最大的单词作为输出\n",
    "                # topi：(batch_size, k)\n",
    "                ni = topi[:, 0]\n",
    "                decoder_input = Variable(ni.unsqueeze(1)) \n",
    "                # decoder_input: (batch_size, length_seq)\n",
    "                decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "                \n",
    "                loss += criterion(decoder_output, target_variable[:, di])  # 累计损失\n",
    "        \n",
    "        loss.backward() # 反向传播\n",
    "        \n",
    "        loss = loss.cpu() if use_cuda else loss\n",
    "        \n",
    "        encoder_optimizer.step() # 梯度下降\n",
    "        decoder_optimizer.step() # 梯度下降\n",
    "        # print(loss.data.numpy())\n",
    "        print_loss_total += loss.data.numpy() # 累加总误差\n",
    "        \n",
    "    print_loss_avg = print_loss_total / len(train_loader)  # 平均误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78e3eb45-6e97-4b80-bcaf-9c30e67149b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_loss = 0\n",
    "rights = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c100d8f9-7910-4ce5-851a-6fe43f9d3cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation():\n",
    "    global valid_loss\n",
    "    global rights\n",
    "    valid_loss = 0\n",
    "    rights = []\n",
    "    \n",
    "    for data in valid_loader: # 验证集\n",
    "        \n",
    "        input_variable = Variable(data[0]).cuda() if use_cuda else Variable(data[0])\n",
    "        # input_variable: (batch_size, length_seq)\n",
    "        target_variable = Variable(data[1]).cuda() if use_cuda else Variable(data[1])\n",
    "        # target_variable: (batch_size, length_seq)\n",
    "        \n",
    "        encoder_hidden = encoder.initHidden(data[0].size()[0])\n",
    "\n",
    "        loss = 0\n",
    "        encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "        # encoder_outputs: (batch_size, length_seq, hidden_size*direction)\n",
    "        # encoder_hidden: (direction*n_layer, batch_size, hidden_size)\n",
    "        \n",
    "        decoder_input = Variable(torch.LongTensor([[sos_token]] * target_variable.size()[0]))\n",
    "        # decoder_input: (batch_size, length_seq)\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "        # decoder_input: (batch_size, length_seq)\n",
    "        \n",
    "        decoder_hidden = encoder_hidden # 继承encoder隐藏单元\n",
    "        # decoder_hidden: (direction*n_layer, batch_size, hidden_size)\n",
    "        \n",
    "        # teacher_forcing_ratio： 用target中的翻译结果作为监督信息 - 50%\n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "        \n",
    "        # 没有教师监督使用解码器自己的预测作为下一时间步的输入\n",
    "        for di in range(MAX_LENGTH): # 对时间步循环\n",
    "\n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "            #decoder_ouput：(batch_size, output_size(input_size))\n",
    "\n",
    "            topv, topi = decoder_output.data.topk(1, dim = 1) # 从输出结果中选择出一个数值最大的单词作为输出\n",
    "            # topi：(batch_size, k)\n",
    "            ni = topi[:, 0]\n",
    "            decoder_input = Variable(ni.unsqueeze(1)) \n",
    "            # decoder_input: (batch_size, length_seq)\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "            right = metrics_acc(decoder_output, target_variable[:, di]) \n",
    "            # right: (猜对个数，总数)\n",
    "            rights.append(right)\n",
    "            \n",
    "            loss += criterion(decoder_output, target_variable[:, di]) # 损失函数\n",
    "            \n",
    "        loss = loss.cpu() if use_cuda else loss\n",
    "        # print(loss)\n",
    "        valid_loss += loss.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "64ec08a2-7bfd-4eb7-bd60-8fae5bed16c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "进程:0, 训练损失:11.3943, 校验损失:13.8531, 词正确率:56.63%\n",
      "进程:1, 训练损失:11.3721, 校验损失:13.8565, 词正确率:57.58%\n",
      "进程:2, 训练损失:11.2954, 校验损失:13.5022, 词正确率:57.58%\n",
      "进程:3, 训练损失:11.2117, 校验损失:13.5722, 词正确率:57.13%\n",
      "进程:4, 训练损失:11.1538, 校验损失:13.6409, 词正确率:56.52%\n",
      "进程:5, 训练损失:11.1451, 校验损失:14.2387, 词正确率:57.42%\n",
      "进程:6, 训练损失:11.0819, 校验损失:13.8633, 词正确率:57.81%\n",
      "进程:7, 训练损失:11.0461, 校验损失:14.0995, 词正确率:57.25%\n",
      "进程:8, 训练损失:11.0188, 校验损失:14.2541, 词正确率:57.58%\n",
      "进程:9, 训练损失:10.9420, 校验损失:13.5143, 词正确率:57.75%\n"
     ]
    }
   ],
   "source": [
    "plot_losses = []\n",
    "num_epoch = 10\n",
    "for epoch in range(num_epoch):\n",
    "    train()\n",
    "    evaluation()\n",
    "    right_ratio = 1.0 * np.sum([i[0] for i in rights]) / np.sum([i[1] for i in rights])\n",
    "    print(f'进程:{epoch}, 训练损失:{print_loss_avg:.4f}, 校验损失:{valid_loss/len(valid_loader):.4f}, 词正确率:{100.0 * right_ratio:.2f}%')\n",
    "    plot_losses.append([print_loss_avg, valid_loss / len(valid_loader), right_ratio])\n",
    "                                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6cc39bd2-7128-4a3c-be79-d141618cfa90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x214ab3f7b50>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 将统计指标绘图\n",
    "a = [i[0] for i in plot_losses]\n",
    "b = [i[1] for i in plot_losses]\n",
    "c = [i[2] * 100 for i in plot_losses]\n",
    "plt.plot(a, '-', label = 'Training Loss',color = '#00338D')\n",
    "plt.plot(b, ':', label = 'Validation Loss', color = 'green')\n",
    "plt.plot(c, '.', label = 'Accuracy', color = 'red')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss & Accuracy')\n",
    "plt.legend(loc='best')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f8d229f0-91d7-4da7-906e-9fae9c50be10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\torch\\serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.sparse.Embedding' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "D:\\anaconda3\\lib\\site-packages\\torch\\serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.rnn.GRU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "D:\\anaconda3\\lib\\site-packages\\torch\\serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.dropout.Dropout' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "D:\\anaconda3\\lib\\site-packages\\torch\\serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "D:\\anaconda3\\lib\\site-packages\\torch\\serialization.py:786: SourceChangeWarning: source code of class 'torch.nn.modules.activation.LogSoftmax' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "# 加载训练过的神经网络模型\n",
    "encoder_ = torch.load('E:\\\\dasein_py\\\\Data Analysis\\\\Deep Learning - Pytorch\\\\Deep Learning - Pytorch\\\\机器翻译的神经网络实现\\\\data\\\\Encoder_normal_cpu.mdl')\n",
    "decoder_ = torch.load('E:\\\\dasein_py\\\\Data Analysis\\\\Deep Learning - Pytorch\\\\Deep Learning - Pytorch\\\\机器翻译的神经网络实现\\\\data\\\\Decoder_normal_cpu.mdl')\n",
    "if use_cuda:\n",
    "    encoder_ = encoder_.cuda()\n",
    "    decoder_ = decoder_.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d82c17b-74a9-47f4-a304-acbfa62a4855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先，在测试集中随机选择20个句子作为测试\n",
    "indices = np.random.choice(range(len(test_X)), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8188794b-dd50-45dd-a5c1-7d773677a78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c'est pratiquement impossible .\n",
      "机器翻译： it's is . .\n",
      "标准翻译： it's nearly impossible .\n",
      "词准确率： 60.0\n",
      "\n",
      "je suis faineant .\n",
      "机器翻译： i'm am .\n",
      "标准翻译： i'm lazy .\n",
      "词准确率： 80.0\n",
      "\n",
      "arrete de geindre .\n",
      "机器翻译： they it .\n",
      "标准翻译： stop whimpering .\n",
      "词准确率： 60.0\n",
      "\n",
      "vous l'avez fait  !\n",
      "机器翻译： you me .\n",
      "标准翻译： you've done it !\n",
      "词准确率： 20.0\n",
      "\n",
      "elles sont medecins .\n",
      "机器翻译： they are .\n",
      "标准翻译： they're doctors .\n",
      "词准确率： 60.0\n",
      "\n",
      "je suis interessee .\n",
      "机器翻译： i'm am .\n",
      "标准翻译： i'm interested .\n",
      "词准确率： 80.0\n",
      "\n",
      "c'est une coincidence .\n",
      "机器翻译： it's a . .\n",
      "标准翻译： that's a coincidence .\n",
      "词准确率： 60.0\n",
      "\n",
      "ils m'ont enleve .\n",
      "机器翻译： they is . .\n",
      "标准翻译： they kidnapped me .\n",
      "词准确率： 60.0\n",
      "\n",
      "j'etais preoccupee .\n",
      "机器翻译： i was it .\n",
      "标准翻译： i was concerned .\n",
      "词准确率： 80.0\n",
      "\n",
      "vous etes inoubliables .\n",
      "机器翻译： you're are .\n",
      "标准翻译： you're unforgettable .\n",
      "词准确率： 80.0\n",
      "\n",
      "je l'ai cause .\n",
      "机器翻译： i like you .\n",
      "标准翻译： i caused this .\n",
      "词准确率： 60.0\n",
      "\n",
      "elle lui pardonna .\n",
      "机器翻译： she she him .\n",
      "标准翻译： she forgave him .\n",
      "词准确率： 80.0\n",
      "\n",
      "elles sont vieilles .\n",
      "机器翻译： they're all .\n",
      "标准翻译： they're old .\n",
      "词准确率： 80.0\n",
      "\n",
      "tom semble perdu .\n",
      "机器翻译： tom is . .\n",
      "标准翻译： tom seems lost .\n",
      "词准确率： 60.0\n",
      "\n",
      "nous sommes perdues .\n",
      "机器翻译： we're are .\n",
      "标准翻译： we're lost .\n",
      "词准确率： 80.0\n",
      "\n",
      "tiens bon  !\n",
      "机器翻译： it is .\n",
      "标准翻译： hang on .\n",
      "词准确率： 60.0\n",
      "\n",
      "sois respectueuse  !\n",
      "机器翻译： be be .\n",
      "标准翻译： be respectful .\n",
      "词准确率： 80.0\n",
      "\n",
      "je deviens folle .\n",
      "机器翻译： i love you .\n",
      "标准翻译： i'm going crazy .\n",
      "词准确率： 40.0\n",
      "\n",
      "c'est laid .\n",
      "机器翻译： it's is .\n",
      "标准翻译： this is ugly .\n",
      "词准确率： 40.0\n",
      "\n",
      "je m'y trouvais .\n",
      "机器翻译： i was . .\n",
      "标准翻译： i was there .\n",
      "词准确率： 80.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for item in indices: # 对每个句子进行循环\n",
    "    data = [test_X[item]]\n",
    "    target = [test_Y[item]]\n",
    "    print(SentenceFromList(input_lang, data[0])) # 原句\n",
    "    input_variable = Variable(torch.LongTensor(data)).cuda() if use_cuda else Variable(torch.LongTensor(data))\n",
    "    # input_variable：(batch_size, length_seq)\n",
    "    target_variable = Variable(torch.LongTensor(target)).cuda() if use_cuda else Variable(torch.LongTensor(target))\n",
    "    # target_variable: (batch_size, length_seq)\n",
    "    # 初始化编码器\n",
    "    encoder_hidden = encoder.initHidden(input_variable.size()[0])\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden) # 编码器开始编码，结果存储到了encoder_hidden中\n",
    "    # encoder_outputs：(batch_size, length_seq, hidden_size*direction)\n",
    "    # encoder_hidden：(direction*n_layer, batch_size, hidden_size)\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[sos_token]] * target_variable.size()[0])) # 将sos作为解码器的第一个输入\n",
    "    # decoder_input：(batch_size, length_seq)\n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden  # 将编码器的隐含层单元数值拷贝给解码器的隐含层单元\n",
    "    # decoder_hidden：(direction*n_layer, batch_size, hidden_size)\n",
    "\n",
    "    # 没有教师指导下的预测: 使用解码器自己的预测作为解码器下一时刻的输入\n",
    "    output_sentence = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "    rights = []\n",
    "   \n",
    "    for di in range(MAX_LENGTH):  # 按照输出字符进行时间步循环\n",
    "\n",
    "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden) # 解码器一个时间步的计算\n",
    "        #decoder_ouput：(batch_size, output_size(input_size))\n",
    "\n",
    "        topv, topi = decoder_output.data.topk(1, dim = 1) # 解码器的输出: 选择第一轴最大项\n",
    "        #topi：（batch_size, k）\n",
    "        ni = topi[:, 0]\n",
    "        decoder_input = Variable(ni.unsqueeze(1))\n",
    "        ni = ni.numpy()[0]\n",
    "\n",
    "        output_sentence.append(ni) # 将本时间步输出的单词编码加到output_sentence里面\n",
    "        # decoder_input：(batch_size, length_seq)\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "        right = metrics_acc(decoder_output, target_variable[:, di])  # 计算输出字符的准确度\n",
    "        rights.append(right)\n",
    "        \n",
    "    sentence = SentenceFromList(output_lang, output_sentence) # result\n",
    "\n",
    "    standard = SentenceFromList(output_lang, target[0]) # label\n",
    "\n",
    "\n",
    "    print('机器翻译：', sentence)\n",
    "    print('标准翻译：', standard)\n",
    "  \n",
    "    right_ratio = 1.0 * np.sum([i[0] for i in rights]) / np.sum([i[1] for i in rights])\n",
    "    print('词准确率：', 100.0 * right_ratio)\n",
    "    print('')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "da1a4822-8074-47a0-bb00-623fc3922e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有效句子对： 104674\n",
      "总单词数:\n",
      "French 25316\n",
      "English 12274\n",
      "训练记录： 94674\n",
      "校验记录： 5000\n",
      "测试记录： 5000\n"
     ]
    }
   ],
   "source": [
    "# ATTENTION mechanism\n",
    "MAX_LENGTH = 10\n",
    "batch_size = 32\n",
    "\n",
    "#对英文做标准化处理\n",
    "pairs = [[LowerEngString(fra), LowerEngString(eng)] for fra, eng in zip(french, english)]\n",
    "\n",
    "# 对句子对做过滤，处理掉那些超过MAX_LENGTH长度的句子\n",
    "input_lang = Language('French')\n",
    "output_lang = Language('English')\n",
    "pairs = [pair for pair in pairs if FilterPair(pair)]\n",
    "print('有效句子对：', len(pairs))\n",
    "\n",
    "# 建立两个字典（中文的和英文的）\n",
    "for pair in pairs:\n",
    "    input_lang.add_sentence(pair[0])\n",
    "    output_lang.add_sentence(pair[1])\n",
    "print(\"总单词数:\")\n",
    "print(input_lang.name, input_lang.n_words)\n",
    "print(output_lang.name, output_lang.n_words)\n",
    "\n",
    "\n",
    "# 形成训练集，首先，打乱所有句子的顺序\n",
    "random_idx = np.random.permutation(range(len(pairs)))\n",
    "pairs = [pairs[i] for i in random_idx]\n",
    "\n",
    "# 将语言转变为单词的编码构成的序列\n",
    "pairs = [IndexFromPair(pair) for pair in pairs]\n",
    "\n",
    "# 形成训练集、校验集和测试集\n",
    "valid_size = len(pairs) // 10\n",
    "if valid_size > 10000:\n",
    "    valid_size = 10000\n",
    "\n",
    "valid_pairs = pairs[-valid_size : -valid_size // 2]\n",
    "test_pairs = pairs[- valid_size // 2 :]\n",
    "pairs = pairs[ : - valid_size]\n",
    "\n",
    "# 利用PyTorch的dataset和dataloader对象，将数据加载到加载器里面，并且自动分批\n",
    "\n",
    "batch_size = 32 #一撮包含30个数据记录，这个数字越大，系统在训练的时候，每一个周期处理的数据就越多，这样处理越快，但总的数据量会减少\n",
    "\n",
    "print('训练记录：', len(pairs))\n",
    "print('校验记录：', len(valid_pairs))\n",
    "print('测试记录：', len(test_pairs))\n",
    "\n",
    "# 形成训练对列表，用于喂给train_dataset\n",
    "pairs_X = [pair[0] for pair in pairs]\n",
    "pairs_Y = [pair[1] for pair in pairs]\n",
    "valid_X = [pair[0] for pair in valid_pairs]\n",
    "valid_Y = [pair[1] for pair in valid_pairs]\n",
    "test_X = [pair[0] for pair in test_pairs]\n",
    "test_Y = [pair[1] for pair in test_pairs]\n",
    "\n",
    "\n",
    "# 形成训练集\n",
    "train_dataset = DataSet.TensorDataset(torch.LongTensor(pairs_X), torch.LongTensor(pairs_Y))\n",
    "# 形成数据加载器\n",
    "train_loader = DataSet.DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers=8)\n",
    "\n",
    "\n",
    "# 校验数据\n",
    "valid_dataset = DataSet.TensorDataset(torch.LongTensor(valid_X), torch.LongTensor(valid_Y))\n",
    "valid_loader = DataSet.DataLoader(valid_dataset, batch_size = batch_size, shuffle = True, num_workers=8)\n",
    "\n",
    "# 测试数据\n",
    "test_dataset = DataSet.TensorDataset(torch.LongTensor(test_X), torch.LongTensor(test_Y))\n",
    "test_loader = DataSet.DataLoader(test_dataset, batch_size = batch_size, shuffle = True, num_workers = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b0737614-71fb-406d-baa5-ec1c63e92ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    '''加入注意力机制的解码器'''\n",
    "    def __init__(self, hidden_size, output_size, n_layers = 1, dropout=0.1, max_length = MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "        self.max_length = max_length\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        \n",
    "        # 注意力网9络\n",
    "        self.attn = nn.Linear(self.hidden_size * (2*n_layers + 1), self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size*3, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        \n",
    "        # bi-rnn\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, bidirectional = True,num_layers = self.n_layers, batch_first = True)\n",
    "        self.out = nn.Linear(self.hidden_size * 2, self.output_size)\n",
    "    \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # batch_first\n",
    "        # input: (batch_size, length_seq)\n",
    "        embedded = self.embedding(input)\n",
    "        # embedded: (batch_size, length_seq, hidden_size)\n",
    "        embedded = embedded[:, 0, :]\n",
    "        # embedded: (batch_size, 1, hidden_size)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # hidden: (direction*n_layer, batch_size, hidden_size)\n",
    "        temp_for_transpose = torch.transpose(hidden, 0, 1).contiguous()\n",
    "        # torch.contiguous()方法首先拷贝了一份张量在内存中的地址，然后将地址按照形状改变后的张量的语义进行排列。\n",
    "        \n",
    "        # temp_for_transpose: (batch_size, direction*n_layer, hidden_size)\n",
    "        temp_for_transpose = temp_for_transpose.view(temp_for_transpose.size()[0], -1)\n",
    "        \n",
    "        # touch.view()方法对张量改变“形状”其实并没有改变张量在内存中真正的形状\n",
    "        hidden_attn = temp_for_transpose\n",
    "        \n",
    "        # hidden_attn: (batch_size, direction*n_layers*hidden_size)\n",
    "        input_to_attention = torch.cat((embedded, hidden_attn), 1)\n",
    "        \n",
    "        # input_to_attention: (batch_size, hidden_size * (1 + direction * n_layers))\n",
    "        attn_weights = F.softmax(self.attn(input_to_attention),dim=1)\n",
    "        # attn_weights：(batch_size, self.max_length)\n",
    "        attn_weights = attn_weights[:, : encoder_outputs.size()[1]] # 当输入数据不标准的时候，对weights截取必要的一段\n",
    "        # attn_weights: (batch_size ,len(encoder_outputs[0])\n",
    "        attn_weights = attn_weights.unsqueeze(1)\n",
    "        # attn_weights: (batch_size , 1, len(encoder_outputs[0])\n",
    "        \n",
    "        # encoder_output：(batch_size, len(encoder_outputs[0]), hidden_size*direction)\n",
    "        attn_applied = torch.bmm(attn_weights, encoder_outputs) \n",
    "        # batch matrix multiply 忽略第一个batch纬度\n",
    "        # attn_applied：(batch_size, 1, hidden_size*direction)\n",
    "        \n",
    "        output = torch.cat((embedded, attn_applied[:,0,:]), 1)\n",
    "        # output: (batch_size, hidden_size*(direction+1))\n",
    "        \n",
    "        output = self.attn_combine(output).unsqueeze(1)\n",
    "        # output：(batch_size, len(encoder_outputs[0], hidden_size)\n",
    "        \n",
    "        output = F.relu(output)\n",
    "        output = self.dropout(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # output：(batch_size, length_seq, hidden_size * directions)\n",
    "        # hidden：(n_layers * directions, batch_size, hidden_size)\n",
    "        output = self.out(output[:, -1, :]) \n",
    "        # output：(batch_size * output_size)\n",
    "        output = F.log_softmax(output, dim = 1) # FC\n",
    "        # output：(batch_size * output_size)\n",
    "        return output, hidden, attn_weights\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        result = Variable(torch.zeros(self.n_layers * 2, batch_size, self.hidden_size))\n",
    "        return result.cuda() if use_cuda else result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "76900e89-2703-48a0-8538-4a618215d750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 重新定义解码器架构即可，其他变量不变\n",
    "hidden_size = 32\n",
    "max_length = MAX_LENGTH\n",
    "n_layers = 1\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size, num_layers = n_layers)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout=0.5,max_length = max_length, n_layers = n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bc69a6-edee-45f1-a5cd-cb3bb73bd03e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
